{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO USE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run  Every cell, skip 4, run testing area\n",
    "### Export dated_all_tables using Pickle\n",
    "### New file created in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trevor O'Hearn\n",
    "#8/4/20\n",
    "#Written to Quickly Scrape the overview table of the Federal Reserve\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import WebScrapingMethods as wsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.federalreserve.gov/releases/h41/'\n",
    "\n",
    "#get list for links\n",
    "soup = wsm.getSoup(base_url)\n",
    "hrefs = soup.select('.col-xs-1 a')\n",
    "end_urls = []\n",
    "for a in hrefs:\n",
    "    #get links for 2020 and 2019\n",
    "    if (a.attrs['href'][:4] == '2020' or a.attrs['href'][:4] == '2019'\n",
    "        or a.attrs['href'][:4] == 'curr'):\n",
    "        end_urls.append(a.attrs['href'] + '/h41.htm')\n",
    "    else:\n",
    "        break\n",
    "\n",
    "urls = wsm.getLinks(base_url, end_url_list=end_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = urls[0]\n",
    "bs = wsm.getSoup(url)\n",
    "date = bs.select('.H41Release td p')[0].text.strip()\n",
    "tables = bs.select('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "actually a table: Reserve Bank credit, related items, andreserve balances of depository institutions at Federal Reserve Banks\n",
      "zipped : 2, 3\n",
      "actually a table: Reserve Bank credit, related items, and reserve balances of depository institutions atFederal Reserve Banks\n",
      "zipped : 3, 3\n",
      "0\n",
      "actually a table: Memorandum item\n",
      "zipped : 5, 3\n",
      "0\n",
      "actually a table: Remaining Maturity\n",
      "zipped : 7, 3\n",
      "0\n",
      "actually a table: Account name\n",
      "zipped : 9, 3\n",
      "0\n",
      "actually a table: Credit Facilities LLCs:\n",
      "zipped : 11, 3\n",
      "0\n",
      "actually a table: Assets, liabilities, and capital\n",
      "zipped : 13, 3\n",
      "actually a table: Assets, liabilities, and capital\n",
      "zipped : 14, 3\n",
      "0\n",
      "actually a table: Assets, liabilities, and capital\n",
      "zipped : 16, 3\n",
      "actually a table: Assets, liabilities, and capital\n",
      "zipped : 17, 3\n",
      "0\n",
      "0\n",
      "actually a table: Federal Reserve notes and collateral\n",
      "zipped : 20, 3\n",
      "0\n",
      "[1, 4, 6, 8, 10, 12, 15, 18, 19, 21]\n"
     ]
    }
   ],
   "source": [
    "bs = wsm.getSoup(urls[0])\n",
    "table_columns = []\n",
    "table_rows = []\n",
    "table_data = {}\n",
    "features = wsm.cleanFeatures(wsm.getFeatures(bs))\n",
    "k = 0\n",
    "failures = []\n",
    "for table in tables:\n",
    "    #get dimensions of table\n",
    "    try:\n",
    "        \n",
    "        k += 1\n",
    "        flag = 0\n",
    "        if (table.select('th', limit=1)[0].attrs['id'].split('c')[1] == '0'):\n",
    "            print('actually a table: {}'.format(table.select('th', limit=1)[0].text))\n",
    "            flag = 1\n",
    "        across = len(table.select('tr')[-1].select('p')) - 1 #can't count the header column\n",
    "        down = len(table.select('td')) // across\n",
    "        bank_bool = False\n",
    "        if (across > 10): #table of banks\n",
    "            bank_bool = True\n",
    "        table_columns.append(across)\n",
    "        table_rows.append(down)\n",
    "        \n",
    "        #get features\n",
    "        table_num = int(table.select('tr th', limit=1)[0].attrs['id'].strip('t').split('c')[0])\n",
    "        features[table_num]\n",
    "        flag = 2\n",
    "        #get data\n",
    "        tds = table.select('td')\n",
    "        data = []\n",
    "        i = 0\n",
    "        while i < down:\n",
    "            text = None\n",
    "            if (bank_bool): #total is in first column\n",
    "                text = tds[across * i].text.strip()\n",
    "            else: #total is last column\n",
    "                text = tds[across * i + across - 1].text.strip()\n",
    "            if (len(text) > 0):\n",
    "                data.append(text)\n",
    "            i += 1\n",
    "        table_data[table_num] = dict(zip(features[table_num], data))\n",
    "        flag = 3\n",
    "        print('zipped : {}, {}'.format(table_num, flag))\n",
    "    except:\n",
    "        print(flag)\n",
    "        failures.append(k)\n",
    "        continue\n",
    "print(failures)\n",
    "#supposed to fail : 0,3,5, 7, 9, 11, 14, 17, 19 (0 indexed)\n",
    "\n",
    "#take dictionaries of tables and make a singulare dictionary\n",
    "all_tables_data = {}\n",
    "for key in table_data:\n",
    "    for k in table_data[key]:\n",
    "        if k in all_tables_data.keys(): #key name already exists\n",
    "            all_tables_data['{} in {}'.format(k, key)] = table_data[key][k]\n",
    "        else:\n",
    "            all_tables_data[k] = table_data[key][k]\n",
    "\n",
    "\n",
    "#take singular dictionary and create dataframe\n",
    "df = pd.DataFrame(data=all_tables_data, index=[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean table data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in table_data:\n",
    "    for key in table_data[k]:\n",
    "        val = table_data[k][key]\n",
    "        val = wsm.removeUnicode(val)\n",
    "        val = wsm.removePlus(val)\n",
    "        val = wsm.removeComma(val)\n",
    "        val = wsm.removeParentheses(val)\n",
    "        try:\n",
    "            val = int(val)\n",
    "        except:\n",
    "            print('int fail')\n",
    "            continue\n",
    "        table_data[k][key] = val\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICTIONARY EXPANDED TO ALL URLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for url in urls:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All URLS INTO DFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "July 2, 2020\n",
      "July 9, 2020\n",
      "July 16, 2020\n",
      "July 23, 2020\n",
      "July 30, 2020\n",
      "June 4, 2020\n",
      "June 11, 2020\n",
      "June 18, 2020\n",
      "June 25, 2020\n",
      "May 7, 2020\n",
      "May 14, 2020\n",
      "May 21, 2020\n",
      "May 28, 2020\n",
      "April 2, 2020\n",
      "April 9, 2020\n",
      "April 16, 2020\n",
      "April 23, 2020\n",
      "April 30, 2020\n",
      "March 5, 2020\n",
      "March 12, 2020\n",
      "March 19, 2020\n",
      "March 26, 2020\n",
      "February 6, 2020\n",
      "February 13, 2020\n",
      "February 20, 2020\n",
      "February 27, 2020\n",
      "January 2, 2020\n",
      "January 9, 2020\n",
      "January 16, 2020\n",
      "January 23, 2020\n",
      "January 30, 2020\n",
      "December 5, 2019\n",
      "December 12, 2019\n",
      "December 19, 2019\n",
      "December 26, 2019\n",
      "November 7, 2019\n",
      "November 14, 2019\n",
      "November 21, 2019\n",
      "November 29, 2019\n",
      "October 3, 2019\n",
      "October 10, 2019\n",
      "October 17, 2019\n",
      "October 24, 2019\n",
      "October 31, 2019\n",
      "September 5, 2019\n",
      "September 12, 2019\n",
      "September 19, 2019\n",
      "September 26, 2019\n",
      "August 1, 2019\n",
      "August 8, 2019\n",
      "August 15, 2019\n",
      "August 22, 2019\n",
      "August 29, 2019\n",
      "July 5, 2019\n",
      "July 11, 2019\n",
      "July 18, 2019\n",
      "July 25, 2019\n",
      "June 6, 2019\n",
      "June 13, 2019\n",
      "June 20, 2019\n",
      "June 27, 2019\n",
      "May 2, 2019\n",
      "May 9, 2019\n",
      "May 16, 2019\n",
      "May 23, 2019\n",
      "May 30, 2019\n",
      "April 4, 2019\n",
      "April 11, 2019\n",
      "April 18, 2019\n",
      "April 25, 2019\n",
      "March 7, 2019\n",
      "March 14, 2019\n",
      "March 21, 2019\n",
      "March 28, 2019\n",
      "February 7, 2019\n",
      "February 14, 2019\n",
      "February 21, 2019\n",
      "February 28, 2019\n",
      "January 3, 2019\n",
      "January 10, 2019\n",
      "January 17, 2019\n",
      "January 24, 2019\n",
      "January 31, 2019\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dfs = []\n",
    "for url in urls:\n",
    "    bs = wsm.getSoup(url)\n",
    "    date = bs.select('.H41Release td p')[0].text.strip()\n",
    "    print(date)\n",
    "    features = wsm.getFeatures(bs)\n",
    "    table = bs.select('table')[1]\n",
    "    tds = table.select('td')\n",
    "    data = []\n",
    "    i = 0\n",
    "    while i < (len(tds) / 4):\n",
    "        text = tds[4 * i + 3].text.strip()\n",
    "        if (len(text) > 0):\n",
    "            data.append(text)\n",
    "        i += 1\n",
    "    cleaned_features = wsm.cleanFeatures(features)\n",
    "\n",
    "    dfvals = {}\n",
    "    dfvals['Date'] = date\n",
    "    for i, f in enumerate(cleaned_features[2]):\n",
    "        dfvals[f] = data[i]\n",
    "\n",
    "    df = pd.DataFrame(data=dfvals, index=[0])\n",
    "    dfs.append(df)\n",
    "\n",
    "#clean dataframe\n",
    "df = dfs[0]\n",
    "for d in dfs[1:]:\n",
    "    df = df.append(d, ignore_index=False)\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%B %d, %Y', errors='ignore')\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "all_dfs = [df]\n",
    "columnNames = df.columns\n",
    "df.fillna('0', inplace=True)\n",
    "#clean data frame\n",
    "for df in all_dfs:\n",
    "    for c in columnNames:\n",
    "        df[c] = df[c].apply(wsm.removeUnicode)\n",
    "        df[c] = df[c].apply(wsm.removePlus)\n",
    "        df[c] = df[c].apply(wsm.removeComma)\n",
    "        df[c] = df[c].astype(int)\n",
    "#df_all.to_csv('quickscrape.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=dfvals, index=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "August 6, 2020\n",
      "July 2, 2020\n",
      "July 9, 2020\n",
      "July 16, 2020\n",
      "July 23, 2020\n",
      "July 30, 2020\n",
      "June 4, 2020\n",
      "June 11, 2020\n",
      "June 18, 2020\n",
      "June 25, 2020\n",
      "May 7, 2020\n",
      "May 14, 2020\n",
      "May 21, 2020\n",
      "May 28, 2020\n",
      "April 2, 2020\n",
      "April 9, 2020\n",
      "April 16, 2020\n",
      "April 23, 2020\n",
      "April 30, 2020\n",
      "March 5, 2020\n",
      "March 12, 2020\n",
      "March 19, 2020\n",
      "March 26, 2020\n",
      "February 6, 2020\n",
      "February 13, 2020\n",
      "February 20, 2020\n",
      "February 27, 2020\n",
      "January 2, 2020\n",
      "January 9, 2020\n",
      "January 16, 2020\n",
      "January 23, 2020\n",
      "January 30, 2020\n",
      "December 5, 2019\n",
      "December 12, 2019\n",
      "December 19, 2019\n",
      "December 26, 2019\n",
      "November 7, 2019\n",
      "November 14, 2019\n",
      "November 21, 2019\n",
      "November 29, 2019\n",
      "October 3, 2019\n",
      "October 10, 2019\n",
      "October 17, 2019\n",
      "October 24, 2019\n",
      "October 31, 2019\n",
      "September 5, 2019\n",
      "September 12, 2019\n",
      "September 19, 2019\n",
      "September 26, 2019\n",
      "August 1, 2019\n",
      "August 8, 2019\n",
      "August 15, 2019\n",
      "August 22, 2019\n",
      "August 29, 2019\n",
      "July 5, 2019\n",
      "July 11, 2019\n",
      "July 18, 2019\n",
      "July 25, 2019\n",
      "June 6, 2019\n",
      "June 13, 2019\n",
      "June 20, 2019\n",
      "June 27, 2019\n",
      "May 2, 2019\n",
      "May 9, 2019\n",
      "May 16, 2019\n",
      "May 23, 2019\n",
      "May 30, 2019\n",
      "April 4, 2019\n",
      "April 11, 2019\n",
      "April 18, 2019\n",
      "April 25, 2019\n",
      "March 7, 2019\n",
      "March 14, 2019\n",
      "March 21, 2019\n",
      "March 28, 2019\n",
      "February 7, 2019\n",
      "February 14, 2019\n",
      "February 21, 2019\n",
      "February 28, 2019\n",
      "January 3, 2019\n",
      "January 10, 2019\n",
      "January 17, 2019\n",
      "January 24, 2019\n",
      "January 31, 2019\n"
     ]
    }
   ],
   "source": [
    "dated_all_tables = {}\n",
    "for url in urls:\n",
    "    bs = wsm.getSoup(url)\n",
    "    date = bs.select('.H41Release td p')[0].text.strip()\n",
    "    table_columns = []\n",
    "    table_rows = []\n",
    "    table_data = {}\n",
    "    features = wsm.cleanFeatures(wsm.getFeatures(bs))\n",
    "    k = 0\n",
    "    failures = []\n",
    "    for table in tables:\n",
    "        #get dimensions of table\n",
    "        try:\n",
    "\n",
    "            k += 1\n",
    "            flag = 0\n",
    "            if (table.select('th', limit=1)[0].attrs['id'].split('c')[1] == '0'):\n",
    "#                print('actually a table: {}'.format(table.select('th', limit=1)[0].text))\n",
    "                flag = 1\n",
    "            across = len(table.select('tr')[-1].select('p')) - 1 #can't count the header column\n",
    "            down = len(table.select('td')) // across\n",
    "            bank_bool = False\n",
    "            if (across > 10): #table of banks\n",
    "                bank_bool = True\n",
    "            table_columns.append(across)\n",
    "            table_rows.append(down)\n",
    "\n",
    "            #get features\n",
    "            table_num = int(table.select('tr th', limit=1)[0].attrs['id'].strip('t').split('c')[0])\n",
    "            features[table_num]\n",
    "            flag = 2\n",
    "            #get data\n",
    "            tds = table.select('td')\n",
    "            data = []\n",
    "            i = 0\n",
    "            while i < down:\n",
    "                text = None\n",
    "                if (bank_bool): #total is in first column\n",
    "                    text = tds[across * i].text.strip()\n",
    "                else: #total is last column\n",
    "                    text = tds[across * i + across - 1].text.strip()\n",
    "                if (len(text) > 0):\n",
    "                    data.append(text)\n",
    "                i += 1\n",
    "            table_data[table_num] = dict(zip(features[table_num], data))\n",
    "            flag = 3\n",
    "#            print('zipped : {}, {}'.format(table_num, flag))\n",
    "        except:\n",
    "#            print(flag)\n",
    "            failures.append(k)\n",
    "            continue\n",
    "#    print(failures)\n",
    "    #supposed to fail : 0,3,5, 7, 9, 11, 14, 17, 19 (0 indexed)\n",
    "\n",
    "    #take dictionaries of tables and make a singulare dictionary\n",
    "    all_tables_data = {}\n",
    "    for key in table_data:\n",
    "        for k in table_data[key]:\n",
    "            if k in all_tables_data.keys(): #key name already exists\n",
    "                all_tables_data['{} in {}'.format(k, key)] = table_data[key][k]\n",
    "            else:\n",
    "                all_tables_data[k] = table_data[key][k]\n",
    "    for k in table_data:\n",
    "        for key in table_data[k]:\n",
    "            val = table_data[k][key]\n",
    "            val = wsm.removeUnicode(val)\n",
    "            val = wsm.removePlus(val)\n",
    "            val = wsm.removeComma(val)\n",
    "            val = wsm.removeParentheses(val)\n",
    "            try:\n",
    "                val = int(val)\n",
    "            except:\n",
    "                continue\n",
    "            table_data[k][key] = val\n",
    "    dated_all_tables[date] = all_tables_data\n",
    "    print(date)\n",
    "    #take singular dictionary and create dataframe\n",
    "    #df = pd.DataFrame(data=all_tables_data, index=[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dated_all_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export with Pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as p\n",
    "\n",
    "p.dump( dated_all_tables, open( \"dated_all_tables.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle as p\n",
    "# p.load( open( \"dated_all_tables.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
